{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><h1>Medallion Architecture con PySpark</h1>\n",
        "<center><h5><b>Versión para Google Colab</b></h3>\n",
        "\n",
        "En esta notebook se trabajará con una serie de archivos .csv y se realizará el proceso de ETL siguiendo la Medallion Architecture.\n",
        "\n",
        "El proceso está adaptado para poder ejecutarse desde Google Colab, en caso de no contar con créditos en Azure para el uso de Databricks.\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "GdA5jQ_mAbU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        " <img src=https://media.licdn.com/dms/image/C4E12AQFDZNNpFYcwLQ/article-inline_image-shrink_1000_1488/0/1628283147919?e=1689206400&v=beta&t=WlxNVNtfJ6qQqIZqT-8PTJxZjQf5bvZkTzTQRb9kKnE width=200px>\n",
        " </p>"
      ],
      "metadata": {
        "id": "IkOyrH9QDaoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1><b>Fase I: De Bronze a Silver</b></h1>\n",
        "<h5>En esta fase se extraerán los datos de la fuente, en este caso en formato .csv . Se subirán a nuestro entorno de trabajo en Colab y, luego de las transformaciones pertinentes, se almacenarán en formato Parquet en la capa silver.</h5>\n",
        "\n",
        "<h3><b>Para comenzar, debemos subir nuestros archivos .csv al almacenamiento de sesión de Google Colab dentro de la carpeta 'bronze'</h3></b></center>"
      ],
      "metadata": {
        "id": "9wqY74xw79mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Instalación de Apache Spark en Google Colab**"
      ],
      "metadata": {
        "id": "61H_YkO7g-l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.png\" width=\"200px\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "AOdYR2nKD4-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Instalamos y actualizamos PySpark"
      ],
      "metadata": {
        "id": "EThOkNYG6ryn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Comando para instalar Y actualizar PySpark\n",
        "!pip install --upgrade pyspark"
      ],
      "metadata": {
        "id": "Ct6Rroz0Ggwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4ba833-8a07-4845-ac41-3043b8661f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 Importamos/creamos la SparkSession. Esto es necesario para poder operar con el framework Apache Spark."
      ],
      "metadata": {
        "id": "TjKGqATzhMyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos SparkSession de PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#La variable 'spark' va a alojar la SparkSession que vamos a crear\n",
        "spark = SparkSession.builder.appName('myAppName').getOrCreate()"
      ],
      "metadata": {
        "id": "L6DGvWKyIC6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 Importamos los datatypes necesarios para armar el schema"
      ],
      "metadata": {
        "id": "_M4oOCNihoQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Datatypes necesarios\n",
        "from pyspark.sql.types import IntegerType, StringType, DoubleType, StructField, StructType"
      ],
      "metadata": {
        "id": "5RrAjsdBbwLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Trabajamos con 'orders.csv' de la capa bronce**"
      ],
      "metadata": {
        "id": "7QImr9cc59Q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaAFeb45X1cH0uyReSZXaPxvs-jvoisalsCw&usqp=CAU width=200px>"
      ],
      "metadata": {
        "id": "7Sn_zwdiEPfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Definimos el path del csv 'orders' y definimos el schema como 'orders_schema'. 'orders_df' va a ser nuestro dataframe."
      ],
      "metadata": {
        "id": "mso-V6r9h1NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definimos el path de 'orders'\n",
        "orders_path = '/content/bronze/orders.csv'\n",
        "\n",
        "#Definimos el schema de 'orders'\n",
        "orders_schema = StructType([\n",
        "                    StructField('ORDER_ID', IntegerType(), False),\n",
        "                    StructField('ORDER_DATETIME', StringType(), False),\n",
        "                    StructField('CUSTOMER_ID', IntegerType(), False),\n",
        "                    StructField('ORDER_STATUS', StringType(), False),\n",
        "                    StructField('STORE_ID', IntegerType(), False)\n",
        "                    ]\n",
        "                    )\n",
        "#Indicamos path del dataframe, tipo de encabezado y el schema, en la variable 'orders_df'\n",
        "orders_df = spark.read.csv(path=orders_path, header=True, schema=orders_schema)"
      ],
      "metadata": {
        "id": "3TKrClBuiKYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Importamos la función 'to_timestamp' que va a ser necesario para castear la columna 'ORDER_DATETIME' y hacemos el casteo en 'ORDER_TIMESTAMP'"
      ],
      "metadata": {
        "id": "6gCBLNb4kqWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importamos 'to_timestamp'\n",
        "from pyspark.sql.functions import to_timestamp"
      ],
      "metadata": {
        "id": "tXrgW9D3lJy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Los cambios se realizarán sobre el mismo 'orders_df'\n",
        "orders_df = orders_df.select('ORDER_ID', \\\n",
        "                             to_timestamp(orders_df['ORDER_DATETIME'], 'dd-MMM-yy kk.mm.ss.SS').alias('ORDER_TIMESTAMP'), \\\n",
        "                             'CUSTOMER_ID', \\\n",
        "                             'ORDER_STATUS', \\\n",
        "                             'STORE_ID'\n",
        "                            )"
      ],
      "metadata": {
        "id": "Ap5rFNkSlTzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Filtramos 'ORDER_STATUS' para que solo queden los que están completados"
      ],
      "metadata": {
        "id": "tFPHs_1vnQzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtramos los 'complete' con .filter\n",
        "orders_df = orders_df.filter(orders_df['ORDER_STATUS']=='COMPLETE')"
      ],
      "metadata": {
        "id": "xxF82sBWnYU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Importar 'stores.csv' y hacer un JOIN, para mostrar los nombres de las tiendas, en lugar de los ID"
      ],
      "metadata": {
        "id": "iL5hjTGwycen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el path de 'stores'\n",
        "stores_path = '/content/bronze/stores.csv'\n",
        "\n",
        "# Definimos el schema de 'stores'\n",
        "stores_schema = StructType([\n",
        "                            StructField('STORE_ID', IntegerType(), False),\n",
        "                            StructField('STORE_NAME', StringType(), False),\n",
        "                            StructField('WEB_ADDRESS', StringType(), False),\n",
        "                            StructField('LATITUDE', DoubleType(), False),\n",
        "                            StructField('LONGITUDE', DoubleType(), False),\n",
        "                            ]\n",
        "                            )\n",
        "\n",
        "# Indicamos el path del dataframe, el tipo de encabezado y el schema en la variable 'stores_df'\n",
        "stores_df = spark.read.csv(path=stores_path, header=True, schema=stores_schema)"
      ],
      "metadata": {
        "id": "cTIJ3LIr0XAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Hacemos un LEFT JOIN de orders con stores para agregar 'store_name' al dataframe. Seleccionamos sólo las columnas necesarias"
      ],
      "metadata": {
        "id": "PpbZ2A3i1yjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mediante '.join' de PySpark podremos realizar un join de las tablas, luego un select para elegir con qué columnas nos quedamos\n",
        "orders_df = orders_df.join(stores_df, orders_df['store_id']==stores_df['store_id'], 'left').select('ORDER_ID','ORDER_TIMESTAMP','CUSTOMER_ID','STORE_NAME')"
      ],
      "metadata": {
        "id": "MdTQN4fV2B2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Sobreescribimos los archivos en la capa 'Silver' como un archivo Parquet"
      ],
      "metadata": {
        "id": "cKh1RhUJ2ljG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El comando '.write' va acompañado del formato en que queremos escribir. En este caso .parquet, pero podría haber sido .csv\n",
        "orders_df.write.parquet('/content/silver/orders', mode='overwrite')"
      ],
      "metadata": {
        "id": "r7RSh9152mbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Trabajamos con 'order_items.csv' de la capa bronce**\n",
        "\n"
      ],
      "metadata": {
        "id": "M988hPRl6YSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaAFeb45X1cH0uyReSZXaPxvs-jvoisalsCw&usqp=CAU width=200px>"
      ],
      "metadata": {
        "id": "1soQR2QjErGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Definimos el path e importamos el schema"
      ],
      "metadata": {
        "id": "yAddammM4aPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el path de 'order_items'\n",
        "order_items_path = '/content/bronze/order_items.csv'\n",
        "\n",
        "# Definimos el schema de 'order_items'\n",
        "order_items_schema = StructType([\n",
        "                                  StructField('ORDER_ID', IntegerType(), False),\n",
        "                                  StructField('LINE_ITEM_ID', StringType(), False),\n",
        "                                  StructField('PRODUCT_ID', IntegerType(), False),\n",
        "                                  StructField('UNIT_PRICE', DoubleType(), False),\n",
        "                                  StructField('QUANTITY', IntegerType(), False)\n",
        "                                ]\n",
        "                                )\n",
        "\n",
        "# Indicamos el path del dataframe, el tipo de encabezado y el schema en la variable 'order_items_df'\n",
        "order_items_df = spark.read.csv(path=order_items_path, header=True, schema=order_items_schema)"
      ],
      "metadata": {
        "id": "2G3YYLBt4f5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Seleccionamos las columnas necesarias y dropeamos las que no vamos a usar"
      ],
      "metadata": {
        "id": "3BV14_WT5SNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# con '.drop' podemos remover las columnas que no necesitemos\n",
        "order_items_df = order_items_df.drop('LINE_ITEM_ID')"
      ],
      "metadata": {
        "id": "_JwwYx8g5W2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Sobreescribimos 'order_items' en la capa Silver como un archivo Parquet"
      ],
      "metadata": {
        "id": "BgZI6kQV5ldl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El comando '.write' va acompañado del formato en que queremos escribir. En este caso .parquet, pero podría haber sido .csv\n",
        "order_items_df.write.parquet('/content/silver/order_items', mode='overwrite')"
      ],
      "metadata": {
        "id": "C1kLUSXW5uDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Trabajamos con 'products.csv' de la capa bronce**"
      ],
      "metadata": {
        "id": "qcoTB70_7U2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaAFeb45X1cH0uyReSZXaPxvs-jvoisalsCw&usqp=CAU width=200px>"
      ],
      "metadata": {
        "id": "u90Q-Q2BEvnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Definimos el path e importamos el schema"
      ],
      "metadata": {
        "id": "tcK86y0B7vaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el path de 'products'\n",
        "products_path = '/content/bronze/products.csv'\n",
        "\n",
        "# Definimos el schema de 'products'\n",
        "products_schema = StructType([\n",
        "                              StructField('PRODUCT_ID', IntegerType(), False),\n",
        "                              StructField('PRODUCT_NAME', StringType(), False),\n",
        "                              StructField('UNIT_PRICE', DoubleType(), False)\n",
        "                             ]\n",
        "                             )\n",
        "\n",
        "# Indicamos el path del dataframe, el tipo de encabezado y el schema en la variable 'order_items_df'\n",
        "products_df = spark.read.csv(path=products_path, header=True, schema=products_schema)"
      ],
      "metadata": {
        "id": "4xyzC9mC7yhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Sobreescribimos 'products' en la capa Silver como un archivo Parquet"
      ],
      "metadata": {
        "id": "H7e6JfRS-ub_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El comando '.write' va acompañado del formato en que queremos escribir. En este caso .parquet, pero podría haber sido .csv\n",
        "products_df.write.parquet('/content/silver/products', mode='overwrite')"
      ],
      "metadata": {
        "id": "zxl3980C-z7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Trabajamos con 'customers.csv' de la capa bronce**"
      ],
      "metadata": {
        "id": "pjXATITI_M2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaAFeb45X1cH0uyReSZXaPxvs-jvoisalsCw&usqp=CAU width=200px>"
      ],
      "metadata": {
        "id": "d2ZsG384FErc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Definimos el path e importamos el schema"
      ],
      "metadata": {
        "id": "77Mav3Fy_Qag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el path de 'customers'\n",
        "customers_path = '/content/bronze/customers.csv'\n",
        "\n",
        "# Definimos el schema de 'customers'\n",
        "customers_schema = StructType([\n",
        "                                StructField('CUSTOMER_ID', IntegerType(), False),\n",
        "                                StructField('FULL_NAME', StringType(), False),\n",
        "                                StructField('EMAIL_ADRESS', StringType(), False)\n",
        "                              ]\n",
        "                              )\n",
        "\n",
        "# Indicamos el path del dataframe, el tipo de encabezado y el schema en la variable 'customers_df'\n",
        "customers_df = spark.read.csv(path=customers_path, header=True, schema=customers_schema)"
      ],
      "metadata": {
        "id": "_LEuf_0h_Y5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Sobreescribimos 'customers' en la capa Silver como un archivo Parquet"
      ],
      "metadata": {
        "id": "A8T0LsNAFeON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El comando '.write' va acompañado del formato en que queremos escribir. En este caso .parquet, pero podría haber sido .csv\n",
        "customers_df.write.parquet('/content/silver/customers', mode='overwrite')"
      ],
      "metadata": {
        "id": "1ukkMKCkFHG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1><b>Fase II: De Silver a Gold</b></h1></center>\n",
        "\n",
        "<center>En esta etapa se realizará el enriquecimiento de los datos previamente tratados. Se importarán los archivos en formato parquet desde la carpeta 'silver', previamente creada.</center>"
      ],
      "metadata": {
        "id": "LsXgYUZGvgzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Definimos el path y leemos los Parquet desde la capa Silver"
      ],
      "metadata": {
        "id": "ODtWsc52xes3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# En las siguientes variables alojaremos la ruta de cada uno de los parquet, a modo de acceso directo\n",
        "customers_silver_path = '/content/silver/customers'\n",
        "order_items_silver_path = '/content/silver/order_items'\n",
        "orders_silver_path = '/content/silver/orders'\n",
        "products_silver_path = '/content/silver/products'\n",
        "\n",
        "# Mediante 'spark.read.parquet' leemos la carpeta donde está alojada cada tabla guardada anteriormente\n",
        "customers_silver_df = spark.read.parquet(customers_silver_path)\n",
        "order_items_silver_df = spark.read.parquet(order_items_silver_path)\n",
        "orders_silver_df = spark.read.parquet(orders_silver_path)\n",
        "products_silver_df = spark.read.parquet(products_silver_path)"
      ],
      "metadata": {
        "id": "5a2NT4rDv0pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>1.1 Comenzamos a trabajar con 'orders', de la capa silver. Podemos hacer un '.show()' para constatar que la información se haya levantado correctamente y un '.dtypes' para comprobar el tipo de dato de cada columna"
      ],
      "metadata": {
        "id": "5vW1O48xAlmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# order_silver_df.show(5)"
      ],
      "metadata": {
        "id": "Bh1Mm4N9A5rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# orders_silver_df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACJZ7ObO75Ho",
        "outputId": "bb528887-3c4c-4b8b-e663-b4a1f6475efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ORDER_ID', 'int'),\n",
              " ('ORDER_TIMESTAMP', 'timestamp'),\n",
              " ('CUSTOMER_ID', 'int'),\n",
              " ('STORE_NAME', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>1.2 Creamos el dataframe 'order_details_df' que contendrá la información de 'orders', pero con la columna 'DATE', utilizando la función 'to_date' de PySpark"
      ],
      "metadata": {
        "id": "TSCL_lIaCaV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la función 'to_date' que será necesaria para realizar casteos\n",
        "from pyspark.sql.functions import to_date"
      ],
      "metadata": {
        "id": "HAKZfKH6CReB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mediante el '.select', elegimos las columnas que vamos a usar y además realizamos el casteo de 'ORDER_TIMESTAMP' a 'DATE'\n",
        "order_details_df = orders_silver_df.select(\n",
        "    'ORDER_ID',\n",
        "    to_date('ORDER_TIMESTAMP').alias('DATE'),\n",
        "    'CUSTOMER_ID',\n",
        "    'STORE_NAME'\n",
        ")"
      ],
      "metadata": {
        "id": "_9Ysv049AZ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>1.3 Hacemos un LEFT JOIN del nuevo dataframe 'order_details' con 'order_items' de la capa silver y seleccionamos únicamente las columnas necesarias"
      ],
      "metadata": {
        "id": "gIRcaZ2oFQW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos el JOIN, declarando el tipo y la condición unir las tablas. Con '.select' agarramos las columnas necesarias de ambas tablas\n",
        "order_details_df = order_details_df.join(\n",
        "    order_items_silver_df,\n",
        "    order_items_silver_df['ORDER_ID'] == order_details_df['ORDER_ID'],\n",
        "    'left'\n",
        ").select(\n",
        "    order_details_df['ORDER_ID'],\n",
        "    order_details_df['DATE'],\n",
        "    order_details_df['STORE_NAME'],\n",
        "    order_details_df['CUSTOMER_ID'],\n",
        "    order_items_silver_df['UNIT_PRICE'],\n",
        "    order_items_silver_df['QUANTITY'])"
      ],
      "metadata": {
        "id": "e2q1qTw4C5tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> 1.4 Creamos una nueva columna llamada 'TOTAL_SALES_AMOUNT', en la que vamos a tener 'UNIT_PRICE' multiplicado por 'QUANTITY'"
      ],
      "metadata": {
        "id": "XqBxmGXuGxTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se realizan los cambios sobre el mismo 'order_details_df'\n",
        "order_details_df = order_details_df.withColumn('TOTAL_SALES_AMOUNT',order_details_df['UNIT_PRICE']*order_details_df['QUANTITY'])"
      ],
      "metadata": {
        "id": "TcFalkXpG7ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Agrupamos el dataframe 'order_details' tomando la suma del monto total y creamos una nueva columna llamada 'TOTAL_ORDER_AMOUNT' con la cantidad de pedidos."
      ],
      "metadata": {
        "id": "Q9iDDZGb09Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se realizan los cambios sobre el mismo 'order_details_df'\n",
        "order_details_df = order_details_df.groupBy('ORDER_ID', 'DATE', 'CUSTOMER_ID', 'STORE_NAME') \\\n",
        "    .sum('TOTAL_SALES_AMOUNT')\\\n",
        "    .withColumnRenamed('sum(TOTAL_SALES_AMOUNT)', 'TOTAL_ORDER_AMOUNT')"
      ],
      "metadata": {
        "id": "41a1diNV1VMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Redondeamos la columna de 'TOTAL_ORDER_AMOUNT' a únicamente dos decimales"
      ],
      "metadata": {
        "id": "Oi3EPUfo2qk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la función 'round', que sirve para redondear un valor float a, por ejemplo, dos decimales\n",
        "from pyspark.sql.functions import round"
      ],
      "metadata": {
        "id": "Pb_GUnme3DDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_details_df = order_details_df.withColumn('TOTAL_ORDER_AMOUNT', round('TOTAL_ORDER_AMOUNT',2))"
      ],
      "metadata": {
        "id": "H4mDLv7L2yx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Guardamos la tabla modificada en la capa gold como 'ORDER_DETAILS'"
      ],
      "metadata": {
        "id": "hbVhErWs4A8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El comando '.write' va acompañado del formato en que queremos escribir. En este caso .parquet, pero podría haber sido .csv\n",
        "order_details_df.write.parquet('/content/gold/order_details', mode='overwrite')"
      ],
      "metadata": {
        "id": "--PsPQyY4ASU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creamos una tabla agregada llamada 'monthly_sales_df', para mostrar el total de ventas por mes y lo guardamos en la capa gold"
      ],
      "metadata": {
        "id": "xQsV_f6y3yed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Creamos una columna que extraiga el mes y el año de la columna 'DATE'"
      ],
      "metadata": {
        "id": "DPiruhSr4yF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos 'date_format', que nos va a servir para especificar el formato de la fecha\n",
        "from pyspark.sql.functions import date_format"
      ],
      "metadata": {
        "id": "rH2TOuUm5I5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Especificamos cómo aparecerá la fecha en 'MONTH_YEAR'\n",
        "sales_with_month = order_details_df.withColumn('MONTH_YEAR', date_format('DATE','yyyy-MM'))"
      ],
      "metadata": {
        "id": "KdAMzPpc376a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Agrupamos, sumamos, redondeamos, ordenamos y seleccionamos datos relacionados con las ventas mensuales."
      ],
      "metadata": {
        "id": "jhzQBA2IGg5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupamos segun 'MONTH_YEAR'\n",
        "monthly_sales_df = sales_with_month.groupBy('MONTH_YEAR') \\\n",
        "    .sum('TOTAL_ORDER_AMOUNT') \\\n",
        "    .withColumn('TOTAL_SALES', round('sum(TOTAL_ORDER_AMOUNT)', 2)) \\\n",
        "    .sort(sales_with_month['MONTH_YEAR'].desc()) \\\n",
        "    .select('MONTH_YEAR', 'TOTAL_SALES')"
      ],
      "metadata": {
        "id": "4fe6O9HN5dJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Sobreescribimos 'monthly_sales' en la capa Gold, en formato Parquet"
      ],
      "metadata": {
        "id": "CDrAIQjM6QJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El comando '.write' va acompañado del formato en que queremos escribir. En este caso .parquet, pero podría haber sido .csv\n",
        "monthly_sales_df.write.parquet('/content/gold/monthly_sales',mode='overwrite')"
      ],
      "metadata": {
        "id": "FBvZ293V6U7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Creamos una tabla agregada llamada 'STORE_MONTHLY_SALES', donde vamos a mostrar el total de ventas por mes de cada tienda y lo guardamos en la capa gold"
      ],
      "metadata": {
        "id": "wMZ-4dk36lMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Hacemos lo mismo que en el punto anterior, pero en este caso agrupamos por 'STORE_NAME'."
      ],
      "metadata": {
        "id": "EXtMGMoQ86v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Agrupamos, sumamos, redondeamos, ordenamos y seleccionamos datos relacionados con las ventas mensuales por tienda\n",
        "store_monthly_sales_df = sales_with_month.groupBy('MONTH_YEAR', 'STORE_NAME') \\\n",
        "    .sum('TOTAL_ORDER_AMOUNT') \\\n",
        "    .withColumn('TOTAL_SALES', round('sum(TOTAL_ORDER_AMOUNT)', 2)) \\\n",
        "    .sort(sales_with_month['MONTH_YEAR'].desc()) \\\n",
        "    .select('MONTH_YEAR', 'STORE_NAME', 'TOTAL_SALES')"
      ],
      "metadata": {
        "id": "myV196e_6wzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Sobreescribimos 'store_monthly_sales' en la capa gold, en formato parquet"
      ],
      "metadata": {
        "id": "mq8xRnNc9FP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # El comando '.write' va acompañado del formato en que queremos escribir. En este caso .parquet, pero podría haber sido .csv\n",
        "store_monthly_sales_df.write.parquet('/content/gold/store_monthly_sales')"
      ],
      "metadata": {
        "id": "EWm6chzH9Exp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center> Resumen </center>"
      ],
      "metadata": {
        "id": "RlnbVO-j83ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center> En esta notebook, se utilizó la arquitectura Medallion para realizar un ETL completo.\n",
        "\n",
        "### <center>Con este enfoque de capas de almacenamiento y procesamiento, se logró mantener una estructura ordenada y escalable para el flujo de datos.</center>"
      ],
      "metadata": {
        "id": "EdYvXqq3Hl-t"
      }
    }
  ]
}
