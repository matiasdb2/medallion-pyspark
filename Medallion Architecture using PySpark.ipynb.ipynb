{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><h1>Medallion Architecture con PySpark</h1>\n",
        "\n",
        "En este notebook se trabajará con una serie de archivos .csv, siguiendo la **Medallion Architecture**.\n",
        "\n",
        "En la **capa bronce** se alojarán los archivos .csv tal cual fueron extraidos del origen.\n",
        "\n",
        "En la **capa silver** se alojarán los archivos en formato parquet, con las modificaciones pertinentes.\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "GdA5jQ_mAbU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        " <img src=https://media.licdn.com/dms/image/C4E12AQFDZNNpFYcwLQ/article-inline_image-shrink_1000_1488/0/1628283147919?e=1689206400&v=beta&t=WlxNVNtfJ6qQqIZqT-8PTJxZjQf5bvZkTzTQRb9kKnE width=200px>\n",
        " </p>"
      ],
      "metadata": {
        "id": "IkOyrH9QDaoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Instalación de Apache Spark en Google Colab**"
      ],
      "metadata": {
        "id": "61H_YkO7g-l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://www.vectorlogo.zone/logos/apache_spark/apache_spark-ar21.png\" width=\"200px\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "AOdYR2nKD4-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Instalamos y actualizamos PySpark"
      ],
      "metadata": {
        "id": "EThOkNYG6ryn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3aDceM_5GXl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe27d05-0986-45be-ba39-c570bfcec646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=b4bdda1b13d294ff06dddc33d4e8a46785611ab40e624102e44351acf8a3e032\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyspark"
      ],
      "metadata": {
        "id": "Ct6Rroz0Ggwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6473fdc8-4cdc-4a8f-aa28-66149b659665"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 Importamos/creamos la SparkSession"
      ],
      "metadata": {
        "id": "TjKGqATzhMyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('myAppName').getOrCreate()"
      ],
      "metadata": {
        "id": "L6DGvWKyIC6-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 Importamos los datatypes necesarios para armar el schema"
      ],
      "metadata": {
        "id": "_M4oOCNihoQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType, StringType, DoubleType, StructField, StructType"
      ],
      "metadata": {
        "id": "5RrAjsdBbwLY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Trabajamos con 'orders.csv' de la capa bronce**"
      ],
      "metadata": {
        "id": "7QImr9cc59Q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaAFeb45X1cH0uyReSZXaPxvs-jvoisalsCw&usqp=CAU width=200px>"
      ],
      "metadata": {
        "id": "7Sn_zwdiEPfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Definimos el path del csv e importamos el schema"
      ],
      "metadata": {
        "id": "mso-V6r9h1NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_path = '/content/bronze/orders.csv'\n",
        "\n",
        "orders_schema = StructType([\n",
        "                    StructField('ORDER_ID', IntegerType(), False),\n",
        "                    StructField('ORDER_DATETIME', StringType(), False),\n",
        "                    StructField('CUSTOMER_ID', IntegerType(), False),\n",
        "                    StructField('ORDER_STATUS', StringType(), False),\n",
        "                    StructField('STORE_ID', IntegerType(), False)\n",
        "                    ]\n",
        "                    )\n",
        "\n",
        "orders_df = spark.read.csv(path=orders_path, header=True, schema=orders_schema)"
      ],
      "metadata": {
        "id": "3TKrClBuiKYp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Importamos la función 'to_timestamp' que va a ser necesario para castear la columna 'ORDER_DATETIME' y hacemos el casteo en 'ORDER_TIMESTAMP'"
      ],
      "metadata": {
        "id": "6gCBLNb4kqWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp"
      ],
      "metadata": {
        "id": "tXrgW9D3lJy_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = orders_df.select('ORDER_ID', \\\n",
        "                             to_timestamp(orders_df['ORDER_DATETIME'], 'dd-MMM-yy kk.mm.ss.SS').alias('ORDER_TIMESTAMP'), \\\n",
        "                             'CUSTOMER_ID', \\\n",
        "                             'ORDER_STATUS', \\\n",
        "                             'STORE_ID'\n",
        "                            )"
      ],
      "metadata": {
        "id": "Ap5rFNkSlTzR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Filtramos 'ORDER_STATUS' para que solo queden los que están completados"
      ],
      "metadata": {
        "id": "tFPHs_1vnQzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = orders_df.filter(orders_df['ORDER_STATUS']=='COMPLETE')"
      ],
      "metadata": {
        "id": "xxF82sBWnYU_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Importar 'stores.csv' y hacer un JOIN, para mostrar los nombres de las tiendas, en lugar de los ID"
      ],
      "metadata": {
        "id": "iL5hjTGwycen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stores_path = '/content/bronze/stores.csv'\n",
        "\n",
        "stores_schema = StructType([\n",
        "                            StructField('STORE_ID', IntegerType(), False),\n",
        "                            StructField('STORE_NAME', StringType(), False),\n",
        "                            StructField('WEB_ADDRESS', StringType(), False),\n",
        "                            StructField('LATITUDE', DoubleType(), False),\n",
        "                            StructField('LONGITUDE', DoubleType(), False),\n",
        "                            ]\n",
        "                            )\n",
        "\n",
        "stores_df = spark.read.csv(path=stores_path, header=True, schema=stores_schema)"
      ],
      "metadata": {
        "id": "cTIJ3LIr0XAn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Hacemos un LEFT JOIN de orders con stores para agregar 'store_name' al dataframe. Seleccionamos sólo las columnas necesarias"
      ],
      "metadata": {
        "id": "PpbZ2A3i1yjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df = orders_df.join(stores_df, orders_df['store_id']==stores_df['store_id'], 'left').select('ORDER_ID','ORDER_TIMESTAMP','CUSTOMER_ID','STORE_NAME')"
      ],
      "metadata": {
        "id": "MdTQN4fV2B2e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Sobreescribimos los archivos en la capa 'Silver' como un archivo Parquet"
      ],
      "metadata": {
        "id": "cKh1RhUJ2ljG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_df.write.parquet('/content/silver/orders', mode='overwrite')"
      ],
      "metadata": {
        "id": "r7RSh9152mbr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Trabajamos con 'order_items.csv' de la capa bronce**\n",
        "\n"
      ],
      "metadata": {
        "id": "M988hPRl6YSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaAFeb45X1cH0uyReSZXaPxvs-jvoisalsCw&usqp=CAU width=200px>"
      ],
      "metadata": {
        "id": "1soQR2QjErGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Definimos el path e importamos el schema"
      ],
      "metadata": {
        "id": "yAddammM4aPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_items_path = '/content/bronze/order_items.csv'\n",
        "\n",
        "order_items_schema = StructType([\n",
        "                                  StructField('ORDER_ID', IntegerType(), False),\n",
        "                                  StructField('LINE_ITEM_ID', StringType(), False),\n",
        "                                  StructField('PRODUCT_ID', IntegerType(), False),\n",
        "                                  StructField('UNIT_PRICE', DoubleType(), False),\n",
        "                                  StructField('QUANTITY', IntegerType(), False)\n",
        "                                ]\n",
        "                                )\n",
        "\n",
        "order_items_df = spark.read.csv(path=order_items_path, header=True, schema=order_items_schema)"
      ],
      "metadata": {
        "id": "2G3YYLBt4f5n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Seleccionamos las columnas necesarias y dropeamos las que no vamos a usar"
      ],
      "metadata": {
        "id": "3BV14_WT5SNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_items_df = order_items_df.drop('LINE_ITEM_ID')"
      ],
      "metadata": {
        "id": "_JwwYx8g5W2S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Sobreescribimos 'order_items' en la capa Silver como un archivo Parquet"
      ],
      "metadata": {
        "id": "BgZI6kQV5ldl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order_items_df.write.parquet('/content/silver/order_items', mode='overwrite')"
      ],
      "metadata": {
        "id": "C1kLUSXW5uDc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Trabajamos con 'products.csv' de la capa bronce**"
      ],
      "metadata": {
        "id": "qcoTB70_7U2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaAFeb45X1cH0uyReSZXaPxvs-jvoisalsCw&usqp=CAU width=200px>"
      ],
      "metadata": {
        "id": "u90Q-Q2BEvnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Definimos el path e importamos el schema"
      ],
      "metadata": {
        "id": "tcK86y0B7vaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_path = '/content/bronze/products.csv'\n",
        "\n",
        "products_schema = StructType([\n",
        "                              StructField('PRODUCT_ID', IntegerType(), False),\n",
        "                              StructField('PRODUCT_NAME', StringType(), False),\n",
        "                              StructField('UNIT_PRICE', DoubleType(), False)\n",
        "                             ]\n",
        "                             )\n",
        "\n",
        "products_df = spark.read.csv(path=products_path, header=True, schema=products_schema)"
      ],
      "metadata": {
        "id": "4xyzC9mC7yhA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Sobreescribimos 'products' en la capa Silver como un archivo Parquet"
      ],
      "metadata": {
        "id": "H7e6JfRS-ub_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "products_df.write.parquet('/content/silver/products', mode='overwrite')"
      ],
      "metadata": {
        "id": "zxl3980C-z7g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Trabajamos con 'customers.csv' de la capa bronce**"
      ],
      "metadata": {
        "id": "pjXATITI_M2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSaAFeb45X1cH0uyReSZXaPxvs-jvoisalsCw&usqp=CAU width=200px>"
      ],
      "metadata": {
        "id": "d2ZsG384FErc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Definimos el path e importamos el schema"
      ],
      "metadata": {
        "id": "77Mav3Fy_Qag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_path = '/content/bronze/customers.csv'\n",
        "\n",
        "customers_schema = StructType([\n",
        "                                StructField('CUSTOMER_ID', IntegerType(), False),\n",
        "                                StructField('FULL_NAME', StringType(), False),\n",
        "                                StructField('EMAIL_ADRESS', StringType(), False)\n",
        "                              ]\n",
        "                              )\n",
        "\n",
        "customers_df = spark.read.csv(path=customers_path, header=True, schema=customers_schema)"
      ],
      "metadata": {
        "id": "_LEuf_0h_Y5d"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Sobreescribimos 'customers' en la capa Silver como un archivo Parquet"
      ],
      "metadata": {
        "id": "A8T0LsNAFeON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_df.write.parquet('/content/silver/customers', mode='overwrite')"
      ],
      "metadata": {
        "id": "1ukkMKCkFHG3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1><b>Fase II: De Silver a Gold</b></h1></center>\n",
        "\n",
        "<center>En esta etapa se realizará el enriquecimiento de los datos previamente tratados. Se importarán los archivos en formato parquet desde la carpeta 'silver', previamente creada."
      ],
      "metadata": {
        "id": "LsXgYUZGvgzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Definimos el path y leemos los Parquet desde la capa Silver"
      ],
      "metadata": {
        "id": "ODtWsc52xes3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_silver_path = '/content/silver/customers'\n",
        "order_items_silver_path = '/content/silver/order_items'\n",
        "orders_silver_path = '/content/silver/orders'\n",
        "products_silver_path = '/content/silver/products'\n",
        "\n",
        "customers_silver_df = spark.read.parquet(customers_silver_path)\n",
        "order_items_silver_df = spark.read.parquet(order_items_silver_path)\n",
        "orders_silver_df = spark.read.parquet(orders_silver_path)\n",
        "products_silver_df = spark.read.parquet(products_silver_path)"
      ],
      "metadata": {
        "id": "5a2NT4rDv0pF"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}